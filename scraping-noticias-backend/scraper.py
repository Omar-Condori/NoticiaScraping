import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import time
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
from database import Database

class NewsScraper:
    def __init__(self):
        """Inicializa el scraper con la base de datos"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.db = Database()
        
        # Crear tablas si no existen
        print("ðŸ“Š Verificando base de datos...")
        self.db.crear_tablas()
        
        # Agregar fuentes de ejemplo si no hay ninguna
        if not self.db.obtener_fuentes():
            print("ðŸ“° Agregando fuentes de ejemplo...")
            self._agregar_fuentes_ejemplo()
    
    def _agregar_fuentes_ejemplo(self):
        """Agrega fuentes de noticias de ejemplo"""
        fuentes_ejemplo = [
            {
                'nombre': 'BBC News',
                'url': 'https://www.bbc.com/news',
                'selector_contenedor': {'name': 'div', 'attrs': {'data-testid': 'card-text-wrapper'}},
                'selector_titulo': {'name': 'h2'},
                'selector_resumen': {'name': 'p'},
                'selector_link': {'name': 'a'},
                'selector_imagen': {'name': 'img'},  # <--- Â¡NUEVO!
                'selector_categoria': {'name': 'span', 'attrs': {'class': 'category'}},  # <--- Â¡NUEVO!
                'activo': True
            },
            {
                'nombre': 'CNN',
                'url': 'https://edition.cnn.com',
                'selector_contenedor': {'name': 'div', 'attrs': {'class': 'container__item'}},
                'selector_titulo': {'name': 'span', 'attrs': {'class': 'container__headline-text'}},
                'selector_resumen': {'name': 'div', 'attrs': {'class': 'container__description'}},
                'selector_link': {'name': 'a'},
                'selector_imagen': {'name': 'img'},  # <--- Â¡NUEVO!
                'selector_categoria': None,  # <--- Â¡NUEVO! CNN puede no tener categorÃ­a visible
                'activo': True
            }
        ]
        
        for fuente in fuentes_ejemplo:
            self.db.agregar_fuente(fuente)
    
    # ==================== GESTIÃ“N DE FUENTES ====================
    
    def agregar_fuente(self, fuente: Dict) -> Optional[Dict]:
        """Agrega una nueva fuente"""
        return self.db.agregar_fuente(fuente)
    
    def obtener_fuentes(self, solo_activas: bool = False) -> List[Dict]:
        """Obtiene todas las fuentes"""
        return self.db.obtener_fuentes(solo_activas)
    
    def obtener_fuente(self, fuente_id: int) -> Optional[Dict]:
        """Obtiene una fuente por ID"""
        return self.db.obtener_fuente(fuente_id)
    
    def actualizar_fuente(self, fuente_id: int, datos: Dict) -> Optional[Dict]:
        """Actualiza una fuente"""
        return self.db.actualizar_fuente(fuente_id, datos)
    
    def eliminar_fuente(self, fuente_id: int) -> bool:
        """Elimina una fuente"""
        return self.db.eliminar_fuente(fuente_id)
    
    # ==================== SCRAPING PROFUNDO ====================
    
    def _scrapear_pagina_individual(self, url: str, titulo_parcial: str = None) -> Dict:
        """
        Hace scraping profundo de una pÃ¡gina individual de noticia
        Retorna un dict con titulo, resumen, imagen_url, fecha_publicacion
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            resultado = {
                'titulo': None,
                'resumen': None,
                'imagen_url': None,
                'fecha_publicacion': None
            }
            
            # ========== EXTRAER TÃTULO ==========
            # Estrategia 1: Meta tags (mÃ¡s confiable)
            for meta_tag in soup.find_all('meta'):
                property_attr = meta_tag.get('property', '') or meta_tag.get('name', '')
                if 'og:title' in property_attr.lower() or 'twitter:title' in property_attr.lower():
                    resultado['titulo'] = meta_tag.get('content', '').strip()
                    break
            
            # Estrategia 2: h1 principal
            if not resultado['titulo']:
                h1_tags = soup.find_all('h1')
                for h1 in h1_tags:
                    h1_text = h1.get_text(strip=True)
                    if h1_text and len(h1_text) > 10:
                        resultado['titulo'] = h1_text
                        break
            
            # Estrategia 3: title tag
            if not resultado['titulo']:
                title_tag = soup.find('title')
                if title_tag:
                    title_text = title_tag.get_text(strip=True)
                    # Limpiar tÃ­tulos que incluyen el nombre del sitio
                    if '|' in title_text:
                        title_text = title_text.split('|')[0].strip()
                    if '-' in title_text and len(title_text.split('-')) > 2:
                        title_text = title_text.split('-')[0].strip()
                    if title_text and len(title_text) > 10:
                        resultado['titulo'] = title_text
            
            # Estrategia 4: Usar tÃ­tulo parcial si existe
            if not resultado['titulo'] and titulo_parcial:
                resultado['titulo'] = titulo_parcial
            
            # ========== EXTRAER IMAGEN ==========
            # Estrategia 1: Meta tags (mÃ¡s confiable)
            for meta_tag in soup.find_all('meta'):
                property_attr = meta_tag.get('property', '') or meta_tag.get('name', '')
                if 'og:image' in property_attr.lower() or 'twitter:image' in property_attr.lower():
                    img_url = meta_tag.get('content', '').strip()
                    if img_url and not img_url.startswith('data:'):
                        resultado['imagen_url'] = urljoin(url, img_url)
                        break
            
            # Estrategia 2: Buscar imagen principal con clases comunes
            if not resultado['imagen_url']:
                for class_pattern in ['featured-image', 'main-image', 'article-image', 'post-image', 'hero-image', 'principal']:
                    img_elem = soup.find('img', attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                    if img_elem:
                        img_url = (img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy-src'))
                        if img_url and not img_url.startswith('data:'):
                            resultado['imagen_url'] = urljoin(url, img_url)
                            break
            
            # Estrategia 3: Primera imagen grande en el contenido
            if not resultado['imagen_url']:
                article_content = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile('content|article|post'))
                if article_content:
                    imgs = article_content.find_all('img', limit=5)
                    for img in imgs:
                        img_url = (img.get('src') or img.get('data-src') or img.get('data-lazy-src'))
                        if img_url and not img_url.startswith('data:'):
                            # Filtrar iconos
                            if not any(icon in img_url.lower() for icon in ['icon', 'logo', 'avatar', 'favicon']):
                                resultado['imagen_url'] = urljoin(url, img_url)
                                break
            
            # ========== EXTRAER RESUMEN/DESCRIPCIÃ“N ==========
            # Estrategia 1: Meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
            if meta_desc:
                desc = meta_desc.get('content', '').strip()
                if desc and len(desc) > 20:
                    resultado['resumen'] = desc[:500]
            
            # Estrategia 2: Primer pÃ¡rrafo del artÃ­culo
            if not resultado['resumen'] or len(resultado['resumen']) < 20:
                article_content = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile('content|article|post|entry'))
                if article_content:
                    paragraphs = article_content.find_all('p')
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        # Filtrar pÃ¡rrafos que son metadata (fechas, autores, etc.)
                        if len(p_text) > 50 and len(p_text) < 1000:
                            # Evitar pÃ¡rrafos que parecen metadata
                            if not any(word in p_text.lower() for word in ['por ', 'publicado', 'actualizado', 'fecha:', 'hora:']):
                                resultado['resumen'] = p_text[:500]
                                break
                    
                    # Si aÃºn no hay resumen, buscar en divs con clases especÃ­ficas
                    if not resultado['resumen'] or len(resultado['resumen']) < 20:
                        for class_pattern in ['lead', 'intro', 'summary', 'excerpt', 'abstract', 'preview', 'description']:
                            desc_div = article_content.find('div', attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                            if desc_div:
                                desc_text = desc_div.get_text(strip=True)
                                if len(desc_text) > 30:
                                    resultado['resumen'] = desc_text[:500]
                                    break
            
            # ========== EXTRAER FECHA DE PUBLICACIÃ“N ==========
            # Estrategia 1: Meta tags
            for meta_tag in soup.find_all('meta'):
                property_attr = meta_tag.get('property', '') or meta_tag.get('name', '')
                if 'published_time' in property_attr.lower() or 'article:published_time' in property_attr.lower():
                    fecha_str = meta_tag.get('content', '').strip()
                    if fecha_str:
                        try:
                            resultado['fecha_publicacion'] = datetime.fromisoformat(fecha_str.replace('Z', '+00:00'))
                        except:
                            pass
                    break
            
            # Estrategia 2: time tag con datetime
            time_tag = soup.find('time', attrs={'datetime': True})
            if time_tag and not resultado['fecha_publicacion']:
                fecha_str = time_tag.get('datetime', '').strip()
                if fecha_str:
                    try:
                        resultado['fecha_publicacion'] = datetime.fromisoformat(fecha_str.replace('Z', '+00:00'))
                    except:
                        pass
            
            # Estrategia 3: Buscar en elementos con clases de fecha
            if not resultado['fecha_publicacion']:
                for class_pattern in ['date', 'fecha', 'published', 'pub-date', 'time', 'timestamp']:
                    date_elem = soup.find(attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        # Intentar parsear diferentes formatos de fecha
                        fecha_parseada = self._parsear_fecha(date_text)
                        if fecha_parseada:
                            resultado['fecha_publicacion'] = fecha_parseada
                            break
            
            return resultado
            
        except Exception as e:
            print(f"      âš ï¸ Error en scraping profundo de {url}: {e}")
            return {
                'titulo': None,
                'resumen': None,
                'imagen_url': None,
                'fecha_publicacion': None
            }
    
    def _parsear_fecha(self, fecha_str: str) -> Optional[datetime]:
        """Intenta parsear diferentes formatos de fecha"""
        if not fecha_str:
            return None
        
        # Formatos comunes
        formatos = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ',
            '%d/%m/%Y %H:%M',
            '%d-%m-%Y %H:%M',
            '%Y-%m-%d',
            '%d/%m/%Y',
        ]
        
        for formato in formatos:
            try:
                return datetime.strptime(fecha_str.strip(), formato)
            except:
                continue
        
        return None
    
    # ==================== SCRAPING ====================
    
    def scrape_fuente(self, fuente: Dict, limite: int = 5, guardar: bool = True) -> List[Dict]:
        """Scrapea noticias de una fuente especÃ­fica"""
        noticias = []
        
        print(f"ðŸ” Scrapeando: {fuente['nombre']}")
        
        try:
            response = requests.get(fuente['url'], headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            contenedor = fuente['selector_contenedor']
            
            # Intentar encontrar artÃ­culos con el selector configurado
            articulos = soup.find_all(
                contenedor['name'],
                contenedor.get('attrs', {}),
                limit=limite
            )
            
            # Si no se encontraron artÃ­culos, intentar selectores alternativos comunes
            if len(articulos) == 0:
                print(f"   âš ï¸ No se encontraron artÃ­culos con el selector configurado, intentando alternativas...")
                # Intentar selectores alternativos comunes
                selectores_alternativos = [
                    {'name': 'article'},
                    {'name': 'div', 'attrs': {'class': re.compile('article|story|news|noticia|post', re.I)}},
                    {'name': 'div', 'attrs': {'itemtype': 'http://schema.org/NewsArticle'}},
                    {'name': 'div', 'attrs': {'data-testid': re.compile('article|story', re.I)}},
                ]
                
                for selector_alt in selectores_alternativos:
                    articulos = soup.find_all(
                        selector_alt['name'],
                        selector_alt.get('attrs', {}),
                        limit=limite
                    )
                    if len(articulos) > 0:
                        print(f"   âœ“ Encontrados {len(articulos)} artÃ­culos con selector alternativo")
                        break
            
            print(f"   Encontrados {len(articulos)} artÃ­culos")
            
            for idx, articulo in enumerate(articulos, 1):
                try:
                    # <--- Â¡MEJORADO! Extraer tÃ­tulo con mÃºltiples estrategias
                    titulo = None
                    
                    # Estrategia 1: Usar selector especÃ­fico
                    titulo_selector = fuente['selector_titulo']
                    titulo_tag = articulo.find(
                        titulo_selector['name'],
                        titulo_selector.get('attrs', {})
                    )
                    if titulo_tag:
                        titulo = titulo_tag.get_text(strip=True)
                    
                    # Estrategia 2: Si no se encontrÃ³, buscar en mÃºltiples lugares comunes
                    if not titulo or titulo == "Sin tÃ­tulo":
                        # Buscar en h1, h2, h3 dentro del artÃ­culo
                        for tag_name in ['h1', 'h2', 'h3', 'h4']:
                            heading = articulo.find(tag_name)
                            if heading:
                                titulo_candidate = heading.get_text(strip=True)
                                if titulo_candidate and len(titulo_candidate) > 5:  # Evitar tÃ­tulos muy cortos
                                    titulo = titulo_candidate
                                    break
                    
                    # Estrategia 3: Buscar en el link (atributo title o texto del link)
                    if not titulo or len(titulo) < 5:
                        link_selector = fuente.get('selector_link', {'name': 'a'})
                        link_tag = articulo.find(
                            link_selector['name'],
                            link_selector.get('attrs', {})
                        )
                        if link_tag:
                            # Intentar obtener del atributo title
                            if link_tag.get('title'):
                                titulo = link_tag.get('title').strip()
                            # Si no, obtener del texto del link
                            elif link_tag.get_text(strip=True):
                                link_text = link_tag.get_text(strip=True)
                                if len(link_text) > 5:  # Solo si tiene suficiente texto
                                    titulo = link_text
                    
                    # Estrategia 4: Buscar en elementos con clases comunes de tÃ­tulo
                    if not titulo or len(titulo) < 5:
                        for class_pattern in ['title', 'headline', 'titulo', 'noticia-titulo', 'entry-title', 'post-title', 'article-title', 'story-title', 'news-title']:
                            title_elem = articulo.find(attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                            if title_elem:
                                titulo_candidate = title_elem.get_text(strip=True)
                                if titulo_candidate and len(titulo_candidate) > 5:
                                    titulo = titulo_candidate
                                    break
                    
                    # Estrategia 5: Buscar en cualquier elemento con atributo title o aria-label
                    if not titulo or len(titulo) < 5:
                        for elem in articulo.find_all(attrs={'title': True}):
                            title_attr = elem.get('title', '').strip()
                            if title_attr and len(title_attr) > 10:
                                titulo = title_attr
                                break
                    
                    # Estrategia 6: Buscar en el primer elemento con texto significativo
                    if not titulo or len(titulo) < 5:
                        # Buscar en divs, spans, etc. con texto largo
                        for tag in ['div', 'span', 'p']:
                            elems = articulo.find_all(tag, limit=10)
                            for elem in elems:
                                text = elem.get_text(strip=True)
                                # Si tiene mÃ¡s de 20 caracteres y menos de 200, probablemente es un tÃ­tulo
                                if 20 < len(text) < 200 and not any(char in text for char in ['http', '@', '.com']):
                                    titulo = text
                                    break
                            if titulo and len(titulo) >= 5:
                                break
                    
                    # Si aÃºn no hay tÃ­tulo, usar fallback
                    if not titulo or len(titulo) < 3:
                        titulo = "Sin tÃ­tulo"
                    
                    # Extraer link
                    link_selector = fuente.get('selector_link', {'name': 'a'})
                    link_tag = articulo.find(
                        link_selector['name'],
                        link_selector.get('attrs', {})
                    )
                    
                    if link_tag and 'href' in link_tag.attrs:
                        url = link_tag['href']
                        if not url.startswith('http'):
                            url = urljoin(fuente['url'], url)
                    else:
                        # Buscar cualquier link dentro del artÃ­culo
                        any_link = articulo.find('a', href=True)
                        if any_link:
                            url = any_link['href']
                            if not url.startswith('http'):
                                url = urljoin(fuente['url'], url)
                        else:
                            url = fuente['url']
                    
                    # <--- Â¡MEJORADO! Extraer resumen con mÃºltiples estrategias
                    resumen = None
                    
                    # Estrategia 1: Usar selector especÃ­fico
                    resumen_selector = fuente['selector_resumen']
                    resumen_tag = articulo.find(
                        resumen_selector['name'],
                        resumen_selector.get('attrs', {})
                    )
                    if resumen_tag:
                        resumen = resumen_tag.get_text(strip=True)
                    
                    # Estrategia 2: Buscar en pÃ¡rrafos comunes
                    if not resumen or resumen == "Sin resumen" or len(resumen) < 10:
                        # Buscar pÃ¡rrafos con clases comunes
                        for class_pattern in ['summary', 'excerpt', 'resumen', 'description', 'descripcion', 'lead']:
                            desc_elem = articulo.find(attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                            if desc_elem:
                                resumen_candidate = desc_elem.get_text(strip=True)
                                if resumen_candidate and len(resumen_candidate) > 10:
                                    resumen = resumen_candidate
                                    break
                    
                    # Estrategia 3: Buscar el primer pÃ¡rrafo con suficiente texto
                    if not resumen or len(resumen) < 10:
                        paragraphs = articulo.find_all('p')
                        for p in paragraphs:
                            p_text = p.get_text(strip=True)
                            # Ignorar pÃ¡rrafos muy cortos (probablemente metadata)
                            if len(p_text) > 20 and len(p_text) < 1000:
                                resumen = p_text
                                break
                    
                    # Estrategia 4: Buscar en divs con clases de descripciÃ³n
                    if not resumen or len(resumen) < 10:
                        for class_pattern in ['lead', 'intro', 'summary', 'excerpt', 'abstract', 'preview']:
                            desc_elem = articulo.find(attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                            if desc_elem:
                                desc_text = desc_elem.get_text(strip=True)
                                if len(desc_text) > 20:
                                    resumen = desc_text
                                    break
                    
                    # Estrategia 5: Buscar en cualquier elemento con atributo data-description o aria-label
                    if not resumen or len(resumen) < 10:
                        for attr in ['data-description', 'aria-label', 'title']:
                            desc_elem = articulo.find(attrs={attr: True})
                            if desc_elem:
                                desc_text = desc_elem.get(attr, '').strip()
                                if len(desc_text) > 20 and len(desc_text) < 500:
                                    resumen = desc_text
                                    break
                    
                    # Si aÃºn no hay resumen, usar fallback
                    if not resumen or len(resumen) < 5:
                        resumen = "Sin resumen"
                    else:
                        resumen = resumen[:500] if len(resumen) > 500 else resumen
                    
                    # <--- Â¡MEJORADO! Extraer imagen con mÃºltiples estrategias
                    imagen_url = None
                    
                    def obtener_url_imagen(img_tag):
                        """Helper para obtener URL de imagen de mÃºltiples atributos"""
                        if not img_tag:
                            return None
                        return (
                            img_tag.get('src') or 
                            img_tag.get('data-src') or 
                            img_tag.get('data-lazy-src') or
                            img_tag.get('data-original') or
                            img_tag.get('data-url') or
                            img_tag.get('data-image') or
                            img_tag.get('data-lazy') or
                            img_tag.get('data-srcset')  # A veces viene en srcset
                        )
                    
                    def es_imagen_valida(img_url, img_tag=None):
                        """Verifica si una imagen es vÃ¡lida (no es icono, no es data URI)"""
                        if not img_url or img_url.startswith('data:'):
                            return False
                        
                        # Filtrar iconos comunes (pero ser menos restrictivo)
                        iconos_comunes = ['favicon', 'sprite']
                        # Solo filtrar si claramente es un icono (no solo contiene la palabra)
                        url_lower = img_url.lower()
                        if any(f'/{icon}.' in url_lower or f'/{icon}/' in url_lower for icon in iconos_comunes):
                            return False
                        
                        # Verificar dimensiones si estÃ¡n disponibles (pero ser mÃ¡s permisivo)
                        if img_tag:
                            width = img_tag.get('width')
                            height = img_tag.get('height')
                            if width and height:
                                try:
                                    w, h = int(width), int(height)
                                    # Filtrar solo imÃ¡genes muy pequeÃ±as (menos de 50x50)
                                    if w < 50 or h < 50:
                                        return False
                                except:
                                    pass
                        
                        return True
                    
                    # Estrategia 1: Usar selector especÃ­fico si existe
                    if fuente.get('selector_imagen'):
                        imagen_selector = fuente['selector_imagen']
                        imagen_tag = articulo.find(
                            imagen_selector['name'],
                            imagen_selector.get('attrs', {})
                        )
                        if imagen_tag:
                            img_url = obtener_url_imagen(imagen_tag)
                            if es_imagen_valida(img_url, imagen_tag):
                                imagen_url = img_url
                    
                    # Estrategia 2: Buscar imÃ¡genes con clases comunes
                    if not imagen_url:
                        for class_pattern in ['image', 'img', 'photo', 'picture', 'foto', 'imagen', 'thumbnail', 'thumb']:
                            img_elem = articulo.find('img', attrs={'class': lambda x: x and class_pattern in str(x).lower()})
                            if img_elem:
                                img_url = obtener_url_imagen(img_elem)
                                if es_imagen_valida(img_url, img_elem):
                                    imagen_url = img_url
                                    break
                    
                    # Estrategia 3: Buscar imÃ¡genes dentro de links (comÃºn en noticias)
                    if not imagen_url:
                        link_with_img = articulo.find('a', href=True)
                        if link_with_img:
                            img_in_link = link_with_img.find('img')
                            if img_in_link:
                                img_url = obtener_url_imagen(img_in_link)
                                if es_imagen_valida(img_url, img_in_link):
                                    imagen_url = img_url
                    
                    # Estrategia 4: Buscar cualquier imagen dentro del artÃ­culo (Ãºltimo recurso)
                    if not imagen_url:
                        img_tags = articulo.find_all('img', limit=5)
                        for img in img_tags:
                            img_url = obtener_url_imagen(img)
                            if es_imagen_valida(img_url, img):
                                imagen_url = img_url
                                break
                    
                    # Estrategia 5: Buscar background-image en estilos (menos comÃºn)
                    if not imagen_url:
                        style_elem = articulo.find(attrs={'style': lambda x: x and 'background-image' in str(x).lower()})
                        if style_elem:
                            style = style_elem.get('style', '')
                            match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
                            if match:
                                bg_url = match.group(1)
                                if es_imagen_valida(bg_url):
                                    imagen_url = bg_url
                    
                    # Convertir URL relativa a absoluta y limpiar
                    if imagen_url:
                        if not imagen_url.startswith('http'):
                            imagen_url = urljoin(fuente['url'], imagen_url)
                        
                        # Limpiar parÃ¡metros de tamaÃ±o si existen (ej: ?w=300&h=200)
                        if '?' in imagen_url:
                            base_url = imagen_url.split('?')[0]
                            # Mantener solo si parece una URL de imagen vÃ¡lida
                            if any(ext in base_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']):
                                imagen_url = base_url
                            else:
                                # Si no tiene extensiÃ³n pero tiene parÃ¡metros, intentar limpiar parÃ¡metros comunes
                                if 'w=' in imagen_url or 'width=' in imagen_url or 'h=' in imagen_url:
                                    # Mantener la URL con parÃ¡metros si no tiene extensiÃ³n clara
                                    pass
                    
                    # <--- Â¡NUEVO! Extraer categorÃ­a
                    categoria = None
                    if fuente.get('selector_categoria'):
                        categoria_selector = fuente['selector_categoria']
                        categoria_tag = articulo.find(
                            categoria_selector['name'],
                            categoria_selector.get('attrs', {})
                        )
                        if categoria_tag:
                            categoria = categoria_tag.get_text(strip=True)
                    
                    # <--- Â¡MEJORADO! Siempre intentar scraping profundo si faltan datos crÃ­ticos
                    fecha_publicacion = None
                    
                    # Determinar si necesitamos scraping profundo
                    necesita_scraping_profundo = (
                        (not titulo or titulo == "Sin tÃ­tulo" or len(titulo) < 5) or
                        (not imagen_url) or
                        (not resumen or resumen == "Sin resumen" or len(resumen) < 20)
                    )
                    
                    # Hacer scraping profundo SIEMPRE que tengamos una URL vÃ¡lida y falten datos
                    if necesita_scraping_profundo and url and url != fuente['url'] and url.startswith('http'):
                        print(f"      ðŸ” Datos incompletos, haciendo scraping profundo de: {url[:60]}...")
                        try:
                            datos_profundos = self._scrapear_pagina_individual(url, titulo if titulo and titulo != "Sin tÃ­tulo" else None)
                            
                            # Usar datos del scraping profundo si son mejores
                            if datos_profundos.get('titulo') and (not titulo or titulo == "Sin tÃ­tulo" or len(titulo) < 5):
                                titulo = datos_profundos['titulo']
                                print(f"         âœ“ TÃ­tulo obtenido del scraping profundo")
                            
                            if datos_profundos.get('imagen_url') and not imagen_url:
                                imagen_url = datos_profundos['imagen_url']
                                print(f"         âœ“ Imagen obtenida del scraping profundo")
                            
                            if datos_profundos.get('resumen') and (not resumen or resumen == "Sin resumen" or len(resumen) < 20):
                                resumen = datos_profundos['resumen']
                                print(f"         âœ“ Resumen obtenido del scraping profundo: {resumen[:50]}...")
                            
                            if datos_profundos.get('fecha_publicacion'):
                                fecha_publicacion = datos_profundos['fecha_publicacion']
                                print(f"         âœ“ Fecha obtenida del scraping profundo")
                            
                            # PequeÃ±a pausa para no sobrecargar el servidor
                            time.sleep(0.3)
                        except Exception as e:
                            print(f"         âš ï¸ Error en scraping profundo: {e}")
                    
                    # Si aÃºn no hay tÃ­tulo vÃ¡lido, usar fallback
                    if not titulo or titulo == "Sin tÃ­tulo" or len(titulo) < 3:
                        titulo = "Sin tÃ­tulo"
                    
                    # Si aÃºn no hay resumen vÃ¡lido, usar fallback
                    if not resumen or resumen == "Sin resumen" or len(resumen) < 5:
                        resumen = "Sin resumen"
                    else:
                        resumen = resumen[:500] if len(resumen) > 500 else resumen
                    
                    noticia = {
                        'titulo': titulo,
                        'url': url,
                        'resumen': resumen,
                        'imagen_url': imagen_url,
                        'categoria': categoria,
                        'fecha_publicacion': fecha_publicacion,
                        'fuente': fuente['nombre'],
                        'fuente_id': fuente['id']
                    }
                    
                    if guardar:
                        noticia_id = self.db.guardar_noticia(noticia)
                        if noticia_id:
                            noticia['id'] = noticia_id
                    
                    noticias.append(noticia)
                    imagen_info = f" [Imagen: {'âœ“' if imagen_url else 'âœ—'}]"
                    categoria_info = f" [CategorÃ­a: {categoria or 'N/A'}]"
                    fecha_info = f" [Fecha: {'âœ“' if fecha_publicacion else 'âœ—'}]"
                    print(f"   âœ“ ArtÃ­culo {idx}: {titulo[:50]}...{imagen_info}{categoria_info}{fecha_info}")
                    
                except Exception as e:
                    print(f"   âœ— Error procesando artÃ­culo {idx}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(f"âœ… {fuente['nombre']}: {len(noticias)} noticias obtenidas\n")
            
        except requests.exceptions.Timeout:
            print(f"â±ï¸  Timeout en {fuente['nombre']}\n")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Error de conexiÃ³n en {fuente['nombre']}: {e}\n")
        except Exception as e:
            print(f"âŒ Error inesperado en {fuente['nombre']}: {e}\n")
        
        return noticias
    
    def scrape_todas_fuentes(self, limite: int = 5, guardar: bool = True, solo_activas: bool = True) -> List[Dict]:
        """Scrapea todas las fuentes activas"""
        print("\n" + "="*60)
        print("ðŸš€ INICIANDO SCRAPING DE NOTICIAS")
        print("="*60 + "\n")
        
        todas = []
        fuentes = self.obtener_fuentes(solo_activas=solo_activas)
        
        print(f"ðŸ“‹ Total de fuentes a scrapear: {len(fuentes)}\n")
        
        for idx, fuente in enumerate(fuentes, 1):
            print(f"[{idx}/{len(fuentes)}] ", end="")
            noticias = self.scrape_fuente(fuente, limite, guardar)
            todas.extend(noticias)
            
            if idx < len(fuentes):
                time.sleep(2)
        
        print("="*60)
        print(f"âœ… SCRAPING COMPLETADO: {len(todas)} noticias totales")
        print("="*60 + "\n")
        
        return todas
    
    # ==================== GESTIÃ“N DE NOTICIAS ====================
    
    # <--- Â¡MODIFICACIÃ“N! Agregar parÃ¡metros offset y categoria
    def obtener_noticias_guardadas(
        self, 
        limite: int = 50, 
        offset: int = 0,  # <--- Â¡NUEVO PARÃMETRO!
        fuente_id: Optional[int] = None,
        categoria: Optional[str] = None  # <--- Â¡NUEVO PARÃMETRO!
    ):
        """Obtiene noticias guardadas en la BD. Retorna (noticias, total)"""
        return self.db.obtener_noticias(limite, offset, fuente_id, categoria)
    
    def contar_noticias(self) -> int:
        """Cuenta el total de noticias guardadas"""
        return self.db.contar_noticias()
    
    def limpiar_noticias(self) -> bool:
        """Elimina todas las noticias de la BD"""
        return self.db.limpiar_noticias()
    
    # <--- Â¡NUEVO MÃ‰TODO! Para obtener categorÃ­as
    def obtener_categorias(self) -> List[str]:
        """Obtiene todas las categorÃ­as Ãºnicas"""
        return self.db.obtener_categorias()