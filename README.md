# üì∞ Sistema de Scraping de Noticias

Sistema completo de scraping de noticias con backend en Flask (Python) y frontend en React. Permite agregar fuentes de noticias, hacer scraping autom√°tico, programar tareas, buscar noticias y exportar datos.

## üöÄ Caracter√≠sticas

- ‚úÖ Scraping autom√°tico de noticias de m√∫ltiples fuentes
- ‚úÖ Autenticaci√≥n JWT
- ‚úÖ Programaci√≥n de tareas de scraping (Scheduler)
- ‚úÖ B√∫squeda avanzada de noticias
- ‚úÖ Estad√≠sticas y an√°lisis
- ‚úÖ Exportaci√≥n de datos (CSV, JSON, TXT)
- ‚úÖ Paginaci√≥n de noticias
- ‚úÖ Filtrado por fuente y categor√≠a
- ‚úÖ Extracci√≥n robusta de t√≠tulos, im√°genes, descripciones y fechas

## üìã Requisitos Previos

### Backend
- Python 3.8 o superior
- PostgreSQL 12 o superior
- pip (gestor de paquetes de Python)

### Frontend
- Node.js 18 o superior
- npm o yarn

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### 1. Clonar el Repositorio

```bash
git clone <url-del-repositorio>
cd NOTICIA
```

### 2. Configurar Base de Datos PostgreSQL

1. **Instalar PostgreSQL** (si no lo tienes):
   ```bash
   # macOS
   brew install postgresql
   brew services start postgresql
   
   # Ubuntu/Debian
   sudo apt-get install postgresql postgresql-contrib
   sudo systemctl start postgresql
   
   # Windows
   # Descargar desde https://www.postgresql.org/download/windows/
   ```

2. **Crear la base de datos**:
   ```bash
   # Conectar a PostgreSQL
   psql -U postgres
   
   # Crear base de datos
   CREATE DATABASE noticias_db;
   
   # Salir
   \q
   ```

3. **Configurar credenciales** en `scraping-noticias-backend/database.py`:
   
   Abre el archivo `scraping-noticias-backend/database.py` y busca la secci√≥n `__init__` de la clase `Database`. Modifica las credenciales:
   
   ```python
   self.config = {
       'host': 'localhost',
       'user': 'postgres',  # Tu usuario de PostgreSQL
       'password': 'tu_password',  # ‚ö†Ô∏è CAMBIA ESTO con tu contrase√±a
       'database': 'noticias_db',
       'port': 5432
   }
   ```
   
   **Nota**: Si no tienes contrase√±a configurada en PostgreSQL, d√©jala como string vac√≠o: `'password': ''`

### 3. Configurar Backend (Python/Flask)

1. **Navegar al directorio del backend**:
   ```bash
   cd scraping-noticias-backend
   ```

2. **Crear entorno virtual** (recomendado):
   ```bash
   python3 -m venv venv
   
   # Activar entorno virtual
   # macOS/Linux:
   source venv/bin/activate
   
   # Windows:
   venv\Scripts\activate
   ```

3. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Verificar que PostgreSQL est√© corriendo**:
   ```bash
   # macOS/Linux
   pg_isready
   
   # O verificar el servicio
   brew services list  # macOS
   sudo systemctl status postgresql  # Linux
   ```

5. **Ejecutar el backend**:
   ```bash
   python app.py
   # o
   python3 app.py
   ```

   El servidor se iniciar√° en `http://localhost:8001`
   La documentaci√≥n Swagger estar√° en `http://localhost:8001/docs`
   
   **Primera ejecuci√≥n**: El sistema crear√° autom√°ticamente las tablas necesarias en la base de datos.

### 4. Configurar Frontend (React)

1. **Abrir una nueva terminal** y navegar al directorio del frontend:
   ```bash
   cd news-scraper-frontend
   ```

2. **Instalar dependencias**:
   ```bash
   npm install
   # o
   yarn install
   ```

3. **Verificar configuraci√≥n de API** en `src/services/api.js`:
   ```javascript
   const API_URL = 'http://localhost:8001/api/v1';
   ```
   Aseg√∫rate de que la URL coincida con la del backend.

4. **Ejecutar el frontend**:
   ```bash
   npm run dev
   # o
   yarn dev
   ```

   La aplicaci√≥n se abrir√° en `http://localhost:5173` (o el puerto que Vite asigne)
   
   **Nota**: Si el puerto 5173 est√° ocupado, Vite usar√° el siguiente disponible (5174, 5175, etc.)

## üìù Uso Inicial

### 1. Crear un Usuario

1. Abre `http://localhost:5173/register`
2. Completa el formulario de registro:
   - Nombre de usuario
   - Email
   - Contrase√±a
3. Ser√°s redirigido autom√°ticamente al dashboard

### 2. Agregar una Fuente de Noticias

1. Ve a la secci√≥n **"Fuentes"** en el men√∫ lateral
2. Haz clic en **"Nueva Fuente"**
3. Completa:
   - **Nombre**: Nombre de la fuente (ej: "RPP Noticias")
   - **URL**: URL del sitio web (ej: "https://rpp.pe")
4. Haz clic en **"Agregar Fuente"**

   ‚ö†Ô∏è **Nota**: Los selectores CSS se asignan autom√°ticamente. Si el scraping no funciona correctamente, puedes editar los selectores manualmente.

### 3. Ejecutar Scraping

1. Ve a la secci√≥n **"Dashboard"**
2. Haz clic en **"Ejecutar Scraping"**
3. Espera a que termine el proceso
4. Las noticias aparecer√°n en la secci√≥n **"Noticias"**

### 4. Ver Noticias

1. Ve a la secci√≥n **"Noticias"**
2. Usa los filtros para:
   - Filtrar por fuente
   - Filtrar por categor√≠a
   - Cambiar el n√∫mero de noticias por p√°gina
3. Navega entre p√°ginas usando los controles de paginaci√≥n

## üîß Soluci√≥n de Problemas

### Backend no se conecta a PostgreSQL

**Error**: `‚ùå Error conectando a PostgreSQL: ...`

**Soluci√≥n**:
1. Verifica que PostgreSQL est√© corriendo:
   ```bash
   pg_isready
   ```
2. Verifica las credenciales en `database.py`
3. Verifica que la base de datos `noticias_db` exista:
   ```bash
   psql -U postgres -l | grep noticias_db
   ```

### Frontend no se conecta al Backend

**Error**: `Failed to fetch` o errores de CORS

**Soluci√≥n**:
1. Verifica que el backend est√© corriendo en `http://localhost:8001`
2. Verifica la URL en `src/services/api.js`
3. Verifica que CORS est√© habilitado en el backend (ya est√° configurado por defecto)

### No se encuentran noticias despu√©s del scraping

**Posibles causas**:
1. Los selectores CSS no son correctos para la fuente
2. La estructura HTML del sitio cambi√≥
3. El sitio bloquea el scraping
4. No se encontraron art√≠culos con el selector configurado

**Soluci√≥n**:
1. Revisa los logs del backend para ver errores
2. El sistema intenta autom√°ticamente selectores alternativos si no encuentra art√≠culos
3. Edita los selectores de la fuente manualmente desde la secci√≥n "Fuentes"
4. Verifica que la URL de la fuente sea correcta y accesible

### El filtro por fuente no muestra noticias

**Causa**: El filtro puede no estar aplic√°ndose correctamente

**Soluci√≥n**:
1. Verifica en los logs del backend que el `fuente_id` se est√© recibiendo correctamente
2. Aseg√∫rate de que las noticias tengan el `fuente_id` correcto en la base de datos
3. Recarga la p√°gina despu√©s de cambiar el filtro
4. Verifica que la fuente seleccionada tenga noticias asociadas

### Las im√°genes no se muestran

**Causa**: Las URLs de im√°genes pueden estar bloqueadas por CORS o ser inv√°lidas

**Soluci√≥n**:
1. El sistema hace scraping profundo autom√°ticamente cuando faltan im√°genes
2. Verifica los logs del backend para ver si se est√°n extrayendo im√°genes
3. Algunas im√°genes pueden requerir configuraci√≥n CORS en el servidor de origen

### Error "Cannot read properties of undefined"

**Causa**: Datos no inicializados correctamente

**Soluci√≥n**:
1. Recarga la p√°gina
2. Verifica que el backend est√© respondiendo correctamente
3. Revisa la consola del navegador para m√°s detalles

## üìÅ Estructura del Proyecto

```
NOTICIA/
‚îú‚îÄ‚îÄ scraping-noticias-backend/    # Backend Flask
‚îÇ   ‚îú‚îÄ‚îÄ app.py                     # Aplicaci√≥n principal
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py                 # L√≥gica de scraping
‚îÇ   ‚îú‚îÄ‚îÄ database.py                # Conexi√≥n y operaciones BD
‚îÇ   ‚îú‚îÄ‚îÄ auth.py                    # Autenticaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py               # Programaci√≥n de tareas
‚îÇ   ‚îú‚îÄ‚îÄ busqueda.py                # B√∫squeda avanzada
‚îÇ   ‚îú‚îÄ‚îÄ estadisticas.py            # Estad√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ exportar.py                # Exportaci√≥n de datos
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python
‚îÇ   ‚îî‚îÄ‚îÄ venv/                      # Entorno virtual (no incluir en git)
‚îÇ
‚îî‚îÄ‚îÄ news-scraper-frontend/          # Frontend React
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ components/            # Componentes React
    ‚îÇ   ‚îú‚îÄ‚îÄ pages/                 # P√°ginas
    ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                 # Custom hooks
    ‚îÇ   ‚îú‚îÄ‚îÄ services/              # Servicios API
    ‚îÇ   ‚îî‚îÄ‚îÄ context/               # Context API
    ‚îú‚îÄ‚îÄ package.json               # Dependencias Node
    ‚îî‚îÄ‚îÄ vite.config.js             # Configuraci√≥n Vite
```

## üîê Variables de Entorno (Opcional)

Para mayor seguridad, puedes usar variables de entorno:

### Backend

Crea un archivo `.env` en `scraping-noticias-backend/`:

```env
DB_HOST=localhost
DB_USER=postgres
DB_PASSWORD=tu_password
DB_NAME=noticias_db
DB_PORT=5432
JWT_SECRET_KEY=tu-super-secreto-cambiar-en-produccion-2025
```

Luego modifica `database.py` para leer estas variables.

### Frontend

Crea un archivo `.env` en `news-scraper-frontend/`:

```env
VITE_API_URL=http://localhost:8001/api/v1
```

## üöÄ Despliegue en Producci√≥n

### Backend

1. **Usar un servidor WSGI** como Gunicorn:
   ```bash
   pip install gunicorn
   gunicorn -w 4 -b 0.0.0.0:8001 app:app
   ```

2. **Usar Nginx como proxy reverso** (recomendado)

3. **Configurar HTTPS** con Let's Encrypt

### Frontend

1. **Construir para producci√≥n**:
   ```bash
   npm run build
   ```

2. **Servir con Nginx o similar**:
   ```bash
   # Los archivos estar√°n en dist/
   # Configurar Nginx para servir desde dist/
   ```

## üìö Endpoints de la API

### Autenticaci√≥n
- `POST /api/v1/auth/register` - Registrar usuario
- `POST /api/v1/auth/login` - Iniciar sesi√≥n
- `GET /api/v1/auth/perfil` - Obtener perfil (requiere JWT)

### Scraping
- `POST /api/v1/scraping/ejecutar` - Ejecutar scraping (requiere JWT)
  - Query params: `limite` (opcional), `fuente_id` (opcional), `guardar` (opcional)

### Noticias
- `GET /api/v1/noticias` - Listar noticias con paginaci√≥n
  - Query params: `limite`, `offset`, `fuente_id` (opcional), `categoria` (opcional)
- `GET /api/v1/noticias/contar` - Contar noticias
- `DELETE /api/v1/noticias` - Eliminar todas las noticias
- `GET /api/v1/noticias/buscar` - B√∫squeda avanzada
- `POST /api/v1/noticias/buscar/palabras-clave` - B√∫squeda por palabras clave
- `GET /api/v1/noticias/exportar` - Exportar noticias (CSV, JSON, TXT)

### Fuentes
- `GET /api/v1/fuentes` - Listar fuentes
  - Query params: `activas` (true/false)
- `POST /api/v1/fuentes` - Agregar fuente (solo requiere `nombre` y `url`)
- `GET /api/v1/fuentes/{id}` - Obtener fuente espec√≠fica
- `PUT /api/v1/fuentes/{id}` - Actualizar fuente
- `DELETE /api/v1/fuentes/{id}` - Eliminar fuente

### Scheduler
- `GET /api/v1/scheduler/tareas` - Listar tareas programadas
- `POST /api/v1/scheduler/tareas` - Crear tarea programada
- `GET /api/v1/scheduler/tareas/{nombre}` - Obtener tarea espec√≠fica
- `DELETE /api/v1/scheduler/tareas/{nombre}` - Eliminar tarea
- `POST /api/v1/scheduler/tareas/{nombre}/pausar` - Pausar tarea
- `POST /api/v1/scheduler/tareas/{nombre}/reanudar` - Reanudar tarea

### Estad√≠sticas
- `GET /api/v1/estadisticas` - Estad√≠sticas generales
- `GET /api/v1/estadisticas/tendencias?dias=7` - Tendencias por d√≠a
- `GET /api/v1/estadisticas/top-fuentes?limite=5` - Top fuentes

### Categor√≠as
- `GET /api/v1/categorias` - Listar todas las categor√≠as

### Documentaci√≥n Completa
Visita `http://localhost:8001/docs` cuando el backend est√© corriendo para ver la documentaci√≥n interactiva de Swagger.

## üêõ Reportar Problemas

Si encuentras alg√∫n problema:

1. **Revisa los logs del backend** en la terminal donde ejecutaste `python app.py`
2. **Revisa la consola del navegador** (F12 ‚Üí Console)
3. **Verifica que todas las dependencias est√©n instaladas**:
   ```bash
   # Backend
   pip list | grep -E "flask|psycopg2|beautifulsoup4"
   
   # Frontend
   npm list react react-dom
   ```
4. **Verifica que PostgreSQL est√© corriendo**:
   ```bash
   pg_isready
   ```
5. **Verifica la conexi√≥n a la base de datos**:
   ```bash
   psql -U postgres -d noticias_db -c "SELECT COUNT(*) FROM noticias;"
   ```

### Problemas Comunes

#### Error: "No module named 'psycopg2'"
```bash
pip install psycopg2-binary
```

#### Error: "Connection refused" en PostgreSQL
```bash
# macOS
brew services start postgresql

# Linux
sudo systemctl start postgresql
```

#### Error: "Cannot read properties of undefined"
- Recarga la p√°gina (F5)
- Verifica que el backend est√© respondiendo
- Revisa la consola del navegador para m√°s detalles

#### Las noticias no se filtran por fuente
- Verifica que el `fuente_id` se est√© enviando correctamente
- Revisa los logs del backend para ver los par√°metros recibidos
- Aseg√∫rate de que las noticias tengan el `fuente_id` correcto

## üìÑ Licencia

Este proyecto es de c√≥digo abierto.

## üë• Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## üìñ Gu√≠a R√°pida de Uso

### Primeros Pasos

1. **Iniciar PostgreSQL**:
   ```bash
   # macOS
   brew services start postgresql
   
   # Linux
   sudo systemctl start postgresql
   ```

2. **Iniciar Backend** (Terminal 1):
   ```bash
   cd scraping-noticias-backend
   source venv/bin/activate  # Si usas entorno virtual
   python app.py
   ```

3. **Iniciar Frontend** (Terminal 2):
   ```bash
   cd news-scraper-frontend
   npm run dev
   ```

4. **Abrir en el navegador**: `http://localhost:5173`

5. **Registrarse**: Crear una cuenta nueva

6. **Agregar fuente**: Ir a "Fuentes" ‚Üí "Nueva Fuente"

7. **Ejecutar scraping**: Ir a "Dashboard" ‚Üí "Ejecutar Scraping"

8. **Ver noticias**: Ir a "Noticias"

## üîç Verificaci√≥n de Instalaci√≥n

### Backend
```bash
cd scraping-noticias-backend
python -c "import flask, psycopg2, bs4; print('‚úÖ Todas las dependencias instaladas')"
```

### Frontend
```bash
cd news-scraper-frontend
npm list react react-dom react-router-dom
```

### Base de Datos
```bash
psql -U postgres -d noticias_db -c "\dt"
# Deber√≠as ver las tablas: usuarios, fuentes, noticias
```

## üìù Notas Importantes

- ‚ö†Ô∏è **Cambiar JWT_SECRET_KEY en producci√≥n**: Edita `app.py` l√≠nea 20
- ‚ö†Ô∏è **Configurar contrase√±a de PostgreSQL**: Edita `database.py` l√≠nea 13
- ‚úÖ El sistema crea las tablas autom√°ticamente en la primera ejecuci√≥n
- ‚úÖ Los selectores CSS se asignan autom√°ticamente al agregar fuentes
- ‚úÖ El scraping profundo se ejecuta autom√°ticamente cuando faltan datos

## üéØ Pr√≥ximos Pasos

Despu√©s de la instalaci√≥n:
1. Agrega tus fuentes de noticias favoritas
2. Configura tareas programadas en "Scheduler"
3. Explora las estad√≠sticas en "Estad√≠sticas"
4. Usa la b√∫squeda avanzada para encontrar noticias espec√≠ficas

---

**Desarrollado con ‚ù§Ô∏è usando Flask, React y PostgreSQL**

**Versi√≥n**: 3.0.0  
**√öltima actualizaci√≥n**: 2025

